{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb769321"
      },
      "source": [
        "# 1. Setup and Data Loading for Pneumonia Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58569a28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d45023d-b310-4a6d-a558-14ef8a9b97dd"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.utils import class_weight\n",
        "import shutil\n",
        "\n",
        "# Check TensorFlow and GPU\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
        "\n",
        "# --- Step 0: Mount ny any eGoogle Drive and Set Paths ---\n",
        "print(\"\\nMounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully!\")\n",
        "\n",
        "# Define the base directory for your dataset in Google Drive.\n",
        "# IMPORTANT: CHANGE THIS PATH!\n",
        "GOOGLE_DRIVE_DATASET_ROOT = '/content/drive/MyDrive/Dataset/chest_xray'\n",
        "BASE_DIR = GOOGLE_DRIVE_DATASET_ROOT\n",
        "\n",
        "# Define directories for training, validation, and testing\n",
        "train_dir = os.path.join(BASE_DIR, 'train')\n",
        "val_dir = os.path.join(BASE_DIR, 'val')\n",
        "test_dir = os.path.join(BASE_DIR, 'test')\n",
        "\n",
        "# Define directories to save models and visualizations\n",
        "MODEL_SAVE_DIR = '/content/drive/MyDrive/pneumonia_project_models'\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "CLASSIFIER_PATH = '/content/drive/MyDrive/my_model_checkpoints/pneumonia_resnet_best_model.h5'\n",
        "VISUALIZATIONS_DIR = '/content/drive/MyDrive/pneumonia_project_visualizations'\n",
        "os.makedirs(VISUALIZATIONS_DIR, exist_ok=True)\n",
        "\n",
        "# Image parameters\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# --- Step 1: Prepare Data Loaders ---\n",
        "print(\"\\n--- Preparing Data Loaders ---\")\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, rotation_range=10, width_shift_range=0.1,\n",
        "    height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest'\n",
        ")\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, class_mode='binary', shuffle=True\n",
        ")\n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    val_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, class_mode='binary', shuffle=False\n",
        ")\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, class_mode='binary', shuffle=False\n",
        ")\n",
        "\n",
        "print(\"\\nClass Indices Mapping:\", train_generator.class_indices)\n",
        "\n",
        "# --- Step 1.5: Address Class Imbalance with Class Weights ---\n",
        "print(\"\\n--- Calculating Class Weights for Imbalanced Data ---\")\n",
        "y_train = train_generator.classes\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "print(\"Class Weights:\", class_weights_dict)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "GPU Available: []\n",
            "\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully!\n",
            "\n",
            "--- Preparing Data Loaders ---\n",
            "Found 5277 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "\n",
            "Class Indices Mapping: {'NORMAL': 0, 'PNEUMONIA': 1}\n",
            "\n",
            "--- Calculating Class Weights for Imbalanced Data ---\n",
            "Class Weights: {np.int32(0): np.float64(1.9529977794226498), np.int32(1): np.float64(0.6720580743759552)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c6fe2ca"
      },
      "source": [
        "# 2. Build and/or Train the Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0a48c13"
      },
      "source": [
        "# --- Step 2: Build or Load the Model ---\n",
        "print(\"\\n--- Building or Loading the Model ---\")\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/my_model_checkpoints/pneumonia_resnet_best_model.h5'\n",
        "\n",
        "# Check if a previously saved model exists in Google Drive\n",
        "if os.path.exists(checkpoint_filepath):\n",
        "    print(f\"Loading previously best saved model from {checkpoint_filepath} to resume training...\")\n",
        "    model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "    if not model.layers[0].trainable: # Assuming the first layer is the base model\n",
        "      print(\"Model is in the frozen base training phase.\")\n",
        "    else:\n",
        "      print(\"Model is in the fine-tuning phase.\")\n",
        "      # It's good practice to re-compile with the low LR if resuming fine-tuning\n",
        "      model.compile(optimizer=Adam(learning_rate=0.000001),\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy',\n",
        "                             tf.keras.metrics.Precision(),\n",
        "                             tf.keras.metrics.Recall(),\n",
        "                             tf.keras.metrics.AUC()])\n",
        "\n",
        "else:\n",
        "    print(\"No previously saved model found. Building a new model from scratch...\")\n",
        "\n",
        "    # Load ResNet50 pre-trained on ImageNet, excluding the top classification layer\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "    base_model.trainable = False # Start with the base frozen\n",
        "\n",
        "    # Add custom classification layers on top\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Compile for the initial frozen base training phase\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy',\n",
        "                           tf.keras.metrics.Precision(),\n",
        "                           tf.keras.metrics.Recall(),\n",
        "                           tf.keras.metrics.AUC()])\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# --- Step 3: Define Callbacks ---\n",
        "print(\"\\n--- Defining Callbacks ---\")\n",
        "# Save the best model to Google Drive\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False, # Save the entire model for easy resuming\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "reduce_lr_on_plateau_callback = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1,\n",
        "    patience=3,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "callbacks = [model_checkpoint_callback, early_stopping_callback, reduce_lr_on_plateau_callback]\n",
        "\n",
        "# --- Step 4: Build, Compile, and Train the Model (Combined Approach) ---\n",
        "print(\"\\n--- Building and Training the Model (Combined Approach) ---\")\n",
        "\n",
        "# Check if a previously saved model exists in Google Drive\n",
        "if os.path.exists(checkpoint_filepath):\n",
        "    print(f\"Loading previously best saved model from {checkpoint_filepath} to resume training...\")\n",
        "    model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "else:\n",
        "    print(\"No previously saved model found. Building a new model from scratch...\")\n",
        "\n",
        "    # Load ResNet50 pre-trained on ImageNet, excluding the top classification layer\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "\n",
        "    # Enable selective fine-tuning from the beginning\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-50]:  # Unfreeze only the last 50 layers\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom classification layers on top\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Recompile the model with a very low learning rate for this combined fine-tuning\n",
        "model.compile(optimizer=Adam(learning_rate=0.000001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy',\n",
        "                       tf.keras.metrics.Precision(),\n",
        "                       tf.keras.metrics.Recall(),\n",
        "                       tf.keras.metrics.AUC()])\n",
        "\n",
        "print(\"\\nModel Summary (Combined Training):\")\n",
        "model.summary()\n",
        "\n",
        "print(\"\\n--- Starting Combined Training ---\")\n",
        "\n",
        "steps_per_epoch_train = train_generator.samples // BATCH_SIZE\n",
        "\n",
        "history_combined = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch_train,\n",
        "    epochs=30,  # Increased epochs for a single training run\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "print(\"\\nCombined training complete. Best model saved to Google Drive.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7966d8b7"
      },
      "source": [
        "# 3. Evaluate the Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48a5430f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d8df58-5cc1-4d6e-e5ab-cc26bd5263c3"
      },
      "source": [
        "# --- Final Evaluation ---\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/my_model_checkpoints/pneumonia_resnet_best_model.h5'\n",
        "model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
        "model.load_weights(checkpoint_filepath)\n",
        "test_loss, test_acc, test_precision, test_recall, test_auc = model.evaluate(test_generator)\n",
        "\n",
        "print(\"\\n--- Test Set Results ---\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation on Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 9s/step - accuracy: 0.7753 - auc_1: 0.6196 - loss: 0.4785 - precision_1: 0.4512 - recall_1: 0.6200\n",
            "\n",
            "--- Test Set Results ---\n",
            "Test Loss: 0.3447\n",
            "Test Accuracy: 0.8446\n",
            "Test Precision: 0.8337\n",
            "Test Recall: 0.9385\n",
            "Test AUC: 0.9302\n"
          ]
        }
      ]
    }
  ]
}